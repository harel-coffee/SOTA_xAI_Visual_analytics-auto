{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1.0-MV-Alibi_exploration.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"C_7VtrUVi8UR"},"source":["# Alibi\n","---\n","Expliration of the library\n","\n","\n","### Model Explanations\n","|Method|Models|Explanations|Classification|Regression|Tabular|Text|Images|Categorical features|Train set required|Distributed|\n","|:---|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---|:---:|\n","|[ALE](https://docs.seldon.io/projects/alibi/en/latest/methods/ALE.html)|BB|global|✔|✔|✔| | | |✔| |\n","|[Anchors](https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html)|BB|local|✔| |✔|✔|✔|✔|For Tabular| |\n","|[CEM](https://docs.seldon.io/projects/alibi/en/latest/methods/CEM.html)|BB* TF/Keras|local|✔| |✔| |✔| |Optional| |\n","|[Counterfactuals](https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html)|BB* TF/Keras|local|✔| |✔| |✔| |No| |\n","|[Prototype Counterfactuals](https://docs.seldon.io/projects/alibi/en/latest/methods/CFProto.html)|BB* TF/Keras|local|✔| |✔| |✔|✔|Optional| |\n","|[Integrated Gradients](https://docs.seldon.io/projects/alibi/en/latest/methods/IntegratedGradients.html)|TF/Keras|local|✔|✔|✔|✔|✔|✔|Optional| |\n","|[Kernel SHAP](https://docs.seldon.io/projects/alibi/en/latest/methods/KernelSHAP.html)|BB|local <br></br>global|✔|✔|✔| | |✔|✔|✔|\n","|[Tree SHAP](https://docs.seldon.io/projects/alibi/en/latest/methods/TreeSHAP.html)|WB|local <br></br>global|✔|✔|✔| | |✔|Optional| | \n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zpJZViAjgWfH","executionInfo":{"status":"ok","timestamp":1625142559081,"user_tz":-120,"elapsed":99605,"user":{"displayName":"Matej V","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiENDeMoSJzyZ5-UMFogmG4cDksNoCQoYPCcFRrxQ=s64","userId":"02305982880680663234"}},"outputId":"e08eba1d-dd60-4c1b-d297-cd0e8a134fbb"},"source":["!pip install alibi[shap]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting alibi[shap]\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/29/d971245c07646cb5b5ed7e9203a7f013573f9e90134d1b8f159cc57ea58d/alibi-0.5.8-py3-none-any.whl (312kB)\n","\u001b[K     |████████████████████████████████| 317kB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: spacy[lookups]<4.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (2.2.4)\n","Requirement already satisfied: dill<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (0.3.4)\n","Requirement already satisfied: typing-extensions>=3.7.2; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (3.7.4.3)\n","Requirement already satisfied: scipy<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (1.4.1)\n","Collecting attrs<21.0.0,>=19.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/aa/cb45262569fcc047bf070b5de61813724d6726db83259222cd7b4c79821a/attrs-20.3.0-py2.py3-none-any.whl (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (0.22.2.post1)\n","Requirement already satisfied: pandas<2.0.0,>=0.23.3 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (1.1.5)\n","Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (2.23.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.16.2 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (1.19.5)\n","Requirement already satisfied: scikit-image!=0.17.1,<0.19,>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (0.16.2)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (3.2.2)\n","Collecting tensorflow<2.5.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/dd/a6e880c0231416eb8ff51bf51e9b04cd08c600c01abc215f33f61cb23e6f/tensorflow-2.4.2-cp37-cp37m-manylinux2010_x86_64.whl (394.5MB)\n","\u001b[K     |████████████████████████████████| 394.5MB 33kB/s \n","\u001b[?25hRequirement already satisfied: Pillow<9.0,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from alibi[shap]) (7.1.2)\n","Collecting shap!=0.38.1,<0.40.0,>=0.36.0; extra == \"shap\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f4/c5b95cddae15be80f8e58b25edceca105aa83c0b8c86a1edad24a6af80d3/shap-0.39.0.tar.gz (356kB)\n","\u001b[K     |████████████████████████████████| 358kB 47.5MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (0.8.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (1.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (1.0.5)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (4.41.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (2.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (3.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (57.0.0)\n","Collecting spacy-lookups-data<0.2.0,>=0.0.5; extra == \"lookups\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/4a37ca7d0c21dc2287a8bb5d249f5f3211cdf3d598acf742bf5bb8c87169/spacy_lookups_data-0.1.0.tar.gz (28.0MB)\n","\u001b[K     |████████████████████████████████| 28.0MB 86kB/s \n","\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25.0,>=0.20.2->alibi[shap]) (1.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi[shap]) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0.0,>=0.23.3->alibi[shap]) (2018.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi[shap]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi[shap]) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi[shap]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.21.0->alibi[shap]) (2021.5.30)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi[shap]) (2.5.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi[shap]) (1.1.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image!=0.17.1,<0.19,>=0.14.2->alibi[shap]) (2.4.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[shap]) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[shap]) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi[shap]) (2.4.7)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.6.3)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.12.1)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.15.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.1.0)\n","Collecting gast==0.3.3\n","  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n","Collecting tensorflow-estimator<2.5.0,>=2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n","\u001b[K     |████████████████████████████████| 471kB 42.3MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (3.3.0)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (2.5.0)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.12.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.36.2)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.2.0)\n","Collecting h5py~=2.10.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 30.9MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.1.2)\n","Collecting grpcio~=1.32.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 24.6MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (3.12.4)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.12)\n","Collecting slicer==0.0.7\n","  Downloading https://files.pythonhosted.org/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap!=0.38.1,<0.40.0,>=0.36.0; extra == \"shap\"->alibi[shap]) (0.51.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap!=0.38.1,<0.40.0,>=0.36.0; extra == \"shap\"->alibi[shap]) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (4.5.0)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image!=0.17.1,<0.19,>=0.14.2->alibi[shap]) (4.4.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.4.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.8.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.31.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap!=0.38.1,<0.40.0,>=0.36.0; extra == \"shap\"->alibi[shap]) (0.34.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy[lookups]<4.0.0,>=2.0.0->alibi[shap]) (3.4.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (4.2.2)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (3.1.1)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5.0,>=2.0.0->alibi[shap]) (0.4.8)\n","Building wheels for collected packages: shap, spacy-lookups-data\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491630 sha256=d1689a0c4a9a7e1f5f6d3869e79c31ace857cf3760ca7e4775c6f28336aac4bf\n","  Stored in directory: /root/.cache/pip/wheels/15/27/f5/a8ab9da52fd159aae6477b5ede6eaaec69fd130fa0fa59f283\n","  Building wheel for spacy-lookups-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.1.0-py2.py3-none-any.whl size=28052158 sha256=26242102658ca952ada44f82dbfe2bbce0e2195f1d9c1717a2b7af55747ec64d\n","  Stored in directory: /root/.cache/pip/wheels/2a/2b/0a/d6fb6235c56d014d224bca760d15d7cbdd820813085ffcd35d\n","Successfully built shap spacy-lookups-data\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: attrs, gast, tensorflow-estimator, h5py, grpcio, tensorflow, slicer, shap, alibi, spacy-lookups-data\n","  Found existing installation: attrs 21.2.0\n","    Uninstalling attrs-21.2.0:\n","      Successfully uninstalled attrs-21.2.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: grpcio 1.34.1\n","    Uninstalling grpcio-1.34.1:\n","      Successfully uninstalled grpcio-1.34.1\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed alibi-0.5.8 attrs-20.3.0 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 shap-0.39.0 slicer-0.0.7 spacy-lookups-data-0.1.0 tensorflow-2.4.2 tensorflow-estimator-2.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k26UoIeGgkgW"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from google.colab import drive\n","import joblib\n","import matplotlib as plt\n","\n","from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve, accuracy_score, confusion_matrix\n","\n","import alibi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zO9AUanmgvs6"},"source":["drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wjs-4ewwifF7"},"source":["path_mv = \"drive/MyDrive/01-Education/03-PhD/2021/Courses/HCI/hci_code/\"\n","path_mg = \"drive/MyDrive/HCI/hci_code/\"\n","path_nj = \"\"\n","\n","# change to your private path\n","gdrive_project_root = path_mv "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3cxp7VSmizBR"},"source":["# Load data and replicate the model"]},{"cell_type":"code","metadata":{"id":"eRSCfAt4iikE"},"source":["X = pd.read_csv(gdrive_project_root + 'data/processed/3_cls_model_input/X_3cls.csv')\n","y = pd.read_csv(gdrive_project_root + 'data/processed/3_cls_model_input/y_3cls.csv')\n","X_train = pd.read_csv(gdrive_project_root + 'data/processed/3_cls_model_input/X_train_3cls.csv')\n","X_test = pd.read_csv(gdrive_project_root + 'data/processed/3_cls_model_input/X_test_3cls.csv')\n","y_train = pd.read_csv(gdrive_project_root + 'data/processed/3_cls_model_input/y_train_3cls.csv')\n","y_test = pd.read_csv(gdrive_project_root + 'data/processed/3_cls_model_input/y_test_3cls.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTDlN0K_iuAW"},"source":["clf = joblib.load(gdrive_project_root+'models/CLF_3classes_GBM-PHC-AASTR_RandomForest_330estimators_42_random_state.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H2UnXSbFivZj"},"source":["predictions = clf.predict(X_test)\n","print(\"Model accuracy: %s \\n\" % accuracy_score(y_test, predictions))\n","print(classification_report(y_test, predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Or6_SmwqixWa"},"source":["plot_confusion_matrix(clf, X_test, y_test,cmap=plt.cm.Blues, display_labels=[\"GBM\", \"PHC\", \"AASTR\"], normalize = \"true\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8xMc6hGjIF3"},"source":["# Accumulated local effects"]},{"cell_type":"code","metadata":{"id":"vY57ub7AjF44"},"source":["from alibi.explainers.ale import ALE, plot_ale\n","target_names = [\"GBM\", \"PHC\", \"AASTR\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-o5THFSyorhM"},"source":["The feature effects cannot be read off immediately because the prediction function includes the effects of all features. What we need is a procedure to block out the effects of all other features to uncover the true effect of RM only. This is exactly what the ALE approach does by averaging the differences of predictions across small intervals of the feature.\n","\n","https://docs.seldon.io/projects/alibi/en/latest/examples/ale_regression_boston.html\n","\n","https://docs.seldon.io/projects/alibi/en/latest/examples/ale_classification.html\n","\n"]},{"cell_type":"code","metadata":{"id":"8Zw2vWprj5W-"},"source":["# logit_fun_lr = clf.decision_function # no decision function in random forest model.. it's probabilistic\n","proba_fun_lr = clf.predict_proba "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucsBYIF4kl1b"},"source":["proba_ale_lr = ALE(proba_fun_lr, feature_names=list(X.columns), target_names=target_names)\n","\n","proba_exp_lr = proba_ale_lr.explain(X_train.values)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XK-ZwgNklL-A"},"source":["plot_ale(proba_exp_lr, n_cols=2, fig_kw={'figwidth': 15, 'figheight': 15});\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ghJY09iAnP0D"},"source":["Note that, in this case, the ALE are in the units of relative probability mass, i.e. given a feature value how much more (less) probability does the model assign to each class relative to the mean prediction. This also means that any increase in relative probability of one class must result into a decrease in probability of another class. In fact, the ALE curves summed across classes result in 0 as a direct consequence of conservation of probability:"]},{"cell_type":"markdown","metadata":{"id":"AhMk4DPcpSe9"},"source":["# Anchors\n","---\n","https://docs.seldon.io/projects/alibi/en/latest/methods/Anchors.html\n"]},{"cell_type":"code","metadata":{"id":"TRf4AYFmp35G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIjN3c6WqPke"},"source":["target_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pv51V96FpVp4"},"source":["predict_fn = lambda x: clf.predict(x)\n","explainer = AnchorTabular(predict_fn, list(X.columns))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"74aD9Zd4qBmy"},"source":["explainer.fit(X_train.values, disc_perc=(25, 50, 75))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKgIlRjkqGZ4"},"source":["idx = 4\n","print('Prediction: ', target_names[explainer.predictor(X_test.values[idx].reshape(1, -1))[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VCURR0VPqsoB"},"source":["for index in range(len(X_test)):\n","  print('--------------%d--------------------' % index)\n","  explanation = explainer.explain(X_test.values[index], threshold=0.95)\n","  print('Prediction: %s' % target_names[explainer.predictor(X_test.values[index].reshape(1, -1))[0] - 1])\n","  print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n","  print('Precision: %.2f' % explanation.precision)\n","  print('Coverage: %.2f' % explanation.coverage)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5eYSZWx3s33O"},"source":["# Contrastive Explanations Method (CEM)\n","---\n","The Contrastive Explanation Method (CEM) can generate black box model explanations in terms of pertinent positives (PP) and pertinent negatives (PN). For PP, it finds what should be minimally and sufficiently present (e.g. important pixels in an image) to justify its classification. PN on the other hand identify what should be minimally and necessarily absent from the explained instance in order to maintain the original prediction.\n","\n","\n","\n","\n","## No implemented visualizations!"]},{"cell_type":"code","metadata":{"id":"ni3aRGev0Bgt"},"source":["from alibi.explainers import CEM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iIi1i6bF1foY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZ-NW_8_s2ge"},"source":["XX = X_test.values[idx].reshape((1,) + X_test.values[idx].shape)\n","print('Prediction on instance to be explained: {}'.format(target_names[np.argmax(clf.predict_proba(XX))]))\n","print('Prediction probabilities for each class on the instance: {}'.format(clf.predict_proba(XX)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGhtQpMSzl3x"},"source":["mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n","shape = (1,) + X_train.values.shape[1:]  # instance shape\n","kappa = .2  # minimum difference needed between the prediction probability for the perturbed instance on the\n","            # class predicted by the original instance and the max probability on the other classes\n","            # in order for the first loss term to be minimized\n","beta = .1  # weight of the L1 loss term\n","c_init = 10.  # initial weight c of the loss term encouraging to predict a different class (PN) or\n","              # the same class (PP) for the perturbed instance compared to the original instance to be explained\n","c_steps = 10  # nb of updates for c\n","max_iterations = 1000  # nb of iterations per value of c\n","feature_range = (X_train.values.min(axis=0).reshape(shape)-.1,  # feature range for the perturbed instance\n","                 X_train.values.max(axis=0).reshape(shape)+.1)  # can be either a float or array of shape (1xfeatures)\n","# clip = (-1001.,1001.)  # gradient clipping\n","clf_init = 1e-2  # initial learning rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dNhGTpGpzzK1"},"source":["# define model\n","predict_fn = lambda x: clf.predict(x)\n","\n","# initialize CEM explainer and explain instance\n","cem = CEM(predict_fn, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range,\n","          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n","          learning_rate_init=clf_init, clip=clip)\n","cem.fit(x_train, no_info_type='median')  # we need to define what feature values contain the least\n","                                         # info wrt predictions\n","                                         # here we will naively assume that the feature-wise median\n","                                         # contains no info; domain knowledge helps!\n","explanation = cem.explain(XX, verbose=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-ElkisG1oqD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0oW20quL1pBQ"},"source":["# Counterfactuals \n","---\n","This method is based on the Interpretable Counterfactual Explanations Guided by Prototypes paper which proposes a fast, model agnostic method to find interpretable counterfactual explanations for classifier predictions by using class prototypes.\n","\n","\n","https://docs.seldon.io/projects/alibi/en/latest/examples/cfproto_housing.html\n","\n","## No implemented visualizations\n"]},{"cell_type":"markdown","metadata":{"id":"nO2shHrj393u"},"source":["# Tree SHAP\n","---\n"]},{"cell_type":"code","metadata":{"id":"UphE7pT552AQ"},"source":["import xgboost as xgb\n","from scipy.special import expit\n","invlogit=expit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OGbkob86nyS"},"source":["def plot_conf_matrix(y_test, y_pred, class_names):\n","    \"\"\"\n","    Plots confusion matrix. Taken from:\n","    http://queirozf.com/entries/visualizing-machine-learning-models-examples-with-scikit-learn-and-matplotlib\n","    \"\"\"\n","\n","    matrix = confusion_matrix(y_test,y_pred)\n","\n","\n","    # place labels at the top\n","    plt.gca().xaxis.tick_top()\n","    plt.gca().xaxis.set_label_position('top')\n","\n","    # plot the matrix per se\n","    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","\n","    # plot colorbar to the right\n","    plt.colorbar()\n","\n","    fmt = 'd'\n","\n","    # write the number of predictions in each bucket\n","    thresh = matrix.max() / 2.\n","    for i, j in product(range(matrix.shape[0]), range(matrix.shape[1])):\n","\n","        # if background is dark, use a white number, and vice-versa\n","        plt.text(j, i, format(matrix[i, j], fmt),\n","             horizontalalignment=\"center\",\n","             color=\"white\" if matrix[i, j] > thresh else \"black\")\n","\n","    tick_marks = np.arange(len(class_names))\n","    plt.xticks(tick_marks, class_names, rotation=45)\n","    plt.yticks(tick_marks, class_names)\n","    plt.tight_layout()\n","    plt.ylabel('True label',size=14)\n","    plt.xlabel('Predicted label',size=14)\n","    plt.show()\n","\n","def predict(xgb_model, dataset, proba=False, threshold=0.5):\n","    \"\"\"\n","    Predicts labels given a xgboost model that outputs raw logits.\n","    \"\"\"\n","\n","    y_pred = model.predict(dataset)  # raw logits are predicted\n","    y_pred_proba = invlogit(y_pred)\n","    if proba:\n","        return y_pred_proba\n","    y_pred_class = np.zeros_like(y_pred)\n","    y_pred_class[y_pred_proba >= threshold] = 1  # assign a label\n","\n","    return y_pred_class"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdAHOD6h6zOO"},"source":["def fit( XXX, yyy, X_valid, y_valid):\n","        print('XGBoost, train data shape        {}'.format(X.shape))\n","        print('XGBoost, validation data shape   {}'.format(X_valid.shape))\n","        print('XGBoost, train labels shape      {}'.format(y.shape))\n","        print('XGBoost, validation labels shape {}'.format(y_valid.shape))\n","        param = {'max_depth': 2, 'eta': 1, 'silent': 1,\n","                             'objective': 'multi:softmax', 'num_class': 3}\n","\n","        train = xgb.DMatrix(data=XXX,\n","                            label=yyy, feature_names=list(X.columns))\n","        valid = xgb.DMatrix(data=X_valid,\n","                            label=y_valid, feature_names=list(X.columns))\n","        estimator = xgb.train(param, dtrain=train,\n","                                   evals=[(train, 'train'), (valid, 'valid')],\n","                                   )\n","        return estimator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADaUvhA26pAE"},"source":["model = fit(X_train.values, y_train.values - 1 , X_test.values, y_test.values-1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TV-Qy5li-g40"},"source":["y_pred_train = predict(model, xgb.DMatrix(\n","    np.ascontiguousarray(X_train),\n","    label=np.ascontiguousarray(y_train),\n","    feature_names=list(X.columns),\n","))\n","y_pred_test = predict(model, xgb.DMatrix(\n","    np.ascontiguousarray(X_test),\n","    label=np.ascontiguousarray(y_test),\n","    feature_names=list(X.columns),\n","))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnDEwOFj_5Ty"},"source":["print(f'Train accuracy:  {round(100*accuracy_score(y_train, y_pred_train), 4)}  %.')\n","print(f'Test  accuracy:  {round(100*accuracy_score(y_test, y_pred_test), 4)}%.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T133BLGmAadh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IqR9CVPh4AKM"},"source":["from functools import partial\n","\n","\n","def _get_importance(model, measure='weight'):\n","    \"\"\"\n","    Retrieves the feature importances from an xgboost\n","    models, measured according to the criterion `measure`.\n","    \"\"\"\n","\n","    imps = model.feature_importances_\n","    names, vals = list(imps.keys()), list(imps.values())\n","    sorter = np.argsort(vals)\n","    s_names, s_vals = tuple(zip(*[(names[i], vals[i]) for i in sorter]))\n","\n","    return s_vals[::-1], s_names[::-1]\n","\n","def plot_importance(feat_imp, feat_names, ax=None, **kwargs):\n","    \"\"\"\n","    Create a horizontal barchart of feature effects, sorted by their magnitude.\n","    \"\"\"\n","\n","    left_x, step ,right_x = kwargs.get(\"left_x\", 0), kwargs.get(\"step\", 50), kwargs.get(\"right_x\")\n","    xticks = np.arange(left_x, right_x, step)\n","    xlabel = kwargs.get(\"xlabel\", 'Feature effects')\n","    xposfactor = kwargs.get(\"xposfactor\", 1)\n","    textfont = kwargs.get(\"text_fontsize\", 25) # 16\n","    yticks_fontsize = kwargs.get(\"yticks_fontsize\", 25)\n","    xlabel_fontsize = kwargs.get(\"xlabel_fontsize\", 30)\n","    textxpos = kwargs.get(\"textxpos\", 60)\n","    textcolor = kwargs.get(\"textcolor\", 'white')\n","\n","    if ax:\n","        fig = None\n","    else:\n","        fig, ax = plt.subplots(figsize=(10, 5))\n","\n","    y_pos = np.arange(len(feat_imp))\n","    ax.barh(y_pos, feat_imp)\n","\n","    ax.set_yticks(y_pos)\n","    ax.set_yticklabels(feat_names, fontsize=yticks_fontsize)\n","    ax.set_xticklabels(xticks, fontsize=30, rotation=45)\n","    ax.invert_yaxis()                  # labels read top-to-bottom\n","    ax.set_xlabel(xlabel, fontsize=xlabel_fontsize)\n","    ax.set_xlim(left=left_x, right=right_x)\n","\n","    for i, v in enumerate(feat_imp):\n","#         if v<0:\n","        textxpos = xposfactor*textxpos\n","        ax.text(v - textxpos, i + .25, str(round(v, 3)), fontsize=textfont, color=textcolor)\n","    return ax, fig\n","\n","get_importance = partial(_get_importance, clf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-6X0kEN4BlZ"},"source":["imp_by_weight_v, imp_by_weight_n = get_importance()\n","imp_by_gain_v, imp_by_gain_n = get_importance(measure='total_gain')\n","imp_by_a_gain_v, imp_by_a_gain_n = get_importance(measure='gain')"],"execution_count":null,"outputs":[]}]}